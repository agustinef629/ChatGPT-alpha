{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Quickstart Guide to Document QA with openai and langchain**\n",
        "*Reading and analyzing journal paper *"
      ],
      "metadata": {
        "id": "CHn2cdWpfNP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain openai chromadb pypdf tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAI5B2bGzaWl",
        "outputId": "9811e35e-265c-48c9-e472-03c188b391d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.267-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb\n",
            "  Downloading chromadb-0.4.6-py3-none-any.whl (405 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.5/405.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf\n",
            "  Downloading pypdf-3.15.1-py3-none-any.whl (271 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.0/271.0 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.21 (from langchain)\n",
            "  Downloading langsmith-0.0.24-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.1.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting pydantic<3,>=1 (from langchain)\n",
            "  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chroma-hnswlib==0.7.2 (from chromadb)\n",
            "  Downloading chroma-hnswlib-0.7.2.tar.gz (31 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fastapi<0.100.0,>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.7.1)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.13.2 (from chromadb)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi<0.100.0,>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.6)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.1.3)\n",
            "Building wheels for collected packages: chroma-hnswlib, pypika\n",
            "  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.2-cp310-cp310-linux_x86_64.whl size=2287473 sha256=8f8c7fd7b98d3df491e2e52b56b2795b42dc094e892860d759de3ef270c84338\n",
            "  Stored in directory: /root/.cache/pip/wheels/11/2b/0d/ee457f6782f75315bb5828d5c2dc5639d471afbd44a830b9dc\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=c95c9111cb508f8ef16ddfafef3866949833ce3ac7e96fc4ab3b7ef598277696\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built chroma-hnswlib pypika\n",
            "Installing collected packages: tokenizers, pypika, monotonic, websockets, uvloop, python-dotenv, pypdf, pydantic, pulsar-client, overrides, mypy-extensions, marshmallow, humanfriendly, httptools, h11, chroma-hnswlib, backoff, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, posthog, openapi-schema-pydantic, langsmith, coloredlogs, openai, onnxruntime, fastapi, dataclasses-json, langchain, chromadb\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.1.1\n",
            "    Uninstalling pydantic-2.1.1:\n",
            "      Successfully uninstalled pydantic-2.1.1\n",
            "Successfully installed backoff-2.2.1 chroma-hnswlib-0.7.2 chromadb-0.4.6 coloredlogs-15.0.1 dataclasses-json-0.5.14 fastapi-0.99.1 h11-0.14.0 httptools-0.6.0 humanfriendly-10.0 langchain-0.0.267 langsmith-0.0.24 marshmallow-3.20.1 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.15.1 openai-0.27.8 openapi-schema-pydantic-1.2.4 overrides-7.4.0 posthog-3.0.2 pulsar-client-3.2.0 pydantic-1.10.12 pypdf-3.15.1 pypika-0.48.9 python-dotenv-1.0.0 starlette-0.27.0 tiktoken-0.4.0 tokenizers-0.13.3 typing-inspect-0.9.0 uvicorn-0.23.2 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.scientifica.uk.com/neurowire/gradhacks-a-guide-to-reading-research-papers"
      ],
      "metadata": {
        "id": "eGVn-3OWEW8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount the drive with the data"
      ],
      "metadata": {
        "id": "gQA6S2kEgrsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIWcZV520ts5",
        "outputId": "a855d33f-d2dc-4eb3-c317-e0224ec48747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Authenticate the API Key"
      ],
      "metadata": {
        "id": "71ByE7dBg0SR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = your_key"
      ],
      "metadata": {
        "id": "Qym4MfYf3Xzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use ChatCompletion model from langchain"
      ],
      "metadata": {
        "id": "XI7OwJo1g2yA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\",openai_api_key = openai.api_key)"
      ],
      "metadata": {
        "id": "c8v5yTEiRGMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PyPDFLoader will extract the text and metadata from each page of the PDF"
      ],
      "metadata": {
        "id": "1kgnXQ959GWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"journal paper.pdf\")"
      ],
      "metadata": {
        "id": "PPi7A9IZyN1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>loader.load() method </b>\n",
        "<li>Returns a list of Document objects</li><li>Each Document contains the page_content field which is the text content of the document</li>\n",
        "\n",
        "We can loop through the list of Documents and print out the page_content to access the text from each document loaded by the loader."
      ],
      "metadata": {
        "id": "r_pPMM4Dg-nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pages = loader.load()"
      ],
      "metadata": {
        "id": "qsJpety19DUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(pages))\n",
        "print(pages)\n",
        "#print(pages[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QrCgbjVyOou",
        "outputId": "e5210f83-8650-45f2-e1f8-ded203789c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n",
            "[Document(page_content='   \\n     \\n   \\n    \\n   \\n    \\n   Int. J. Data Analysis Techniques and Strategies, Vol. 10, No. 4, 2018 369    \\n \\n   Copyright © 2018 Inderscience Enterprises Ltd. \\n     \\n   \\n    \\n   \\n    \\n       \\n A lexicon-based term weighting scheme for emotion \\nidentification of tweets \\nS. Lovelyn Rose* and R. Venkatesan \\nDepartment of CSE, \\nPSG College of Technology, Coimbatore, India Email: lovelyndavid@gmail.com Email: ramanvenkatesan@yahoo.com \\n*Corresponding author \\nGirish Pasupathy \\nDepartment of Windows Server and System Center (WSSC), \\nMicrosoft India Development Center, India \\nEmail: girishsub485@gmail.com \\nP. Swaradh \\nDepartment of CSE, \\nKMCT College of Engineering, Calicut, India \\nEmail: swaradh@gmail.com \\nAbstract:  Detecting emotions in tweets is  a huge challenge due to its li mited \\n140 characters and extensive use of twitter language with evolv ing terms and \\nslangs. This paper uses variou s preprocessing techniques, forms  a  f e a t u r e  \\nvector using lexicons and classifi es tweets into Paul Ekman’s b asic emotions \\nnamely, happy, sad, anger, fear, disgust and surprise using mac hine learning. \\nPreprocessing is done using the d ictionaries ava ilable for emot icons, \\ninterjections and slangs and by h andling punctuation marks and hashtags. The \\nfeature vector is created by combining words from the NRC Emoti on lexicon, \\nWordNet-Affect and online thesaurus. Feature vectors are assign ed weight \\nbased on the presence of punctua tions and negations in the feat ure and the \\ntweets are classified using naiv e Bayes, SVM and random forests . The use of \\nlexicon features and a novel wei ghting scheme has produced a co nsiderable \\ngain in terms of accuracy with random forest achieving maximum accuracy of \\nalmost 73%. \\nKeywords:  emotion classification; Twitter ; preprocessing; feature select ion; \\ndictionaries; lexic ons; term weighting; random forest; SVM; nai ve Bayes. \\nReference  to this paper should be made as follows: Rose, S.L., Venkatesa n, R., \\nPasupathy, G. and Swaradh, P. (2 018) ‘A lexicon-based term weig hting scheme \\nfor emotion identification of tweets’, Int. J. Data Analysis Techniques and \\nStrategies , Vol. 10, No. 4, pp.369–380. \\n ', metadata={'source': 'journal paper.pdf', 'page': 0}), Document(page_content='   \\n    \\n   \\n    \\n   \\n    \\n   370 S.L. Rose et al.    \\n \\n    \\n     \\n   \\n    \\n   \\n    \\n       \\n Biographical notes:  S. Lovelyn Rose is an Associate Professor at the \\nDepartment of CSE in PSG College  of Technology, Coimbatore. She  graduated \\nin Mathematics and subsequently acquired two post-graduate degr ees namely \\nin Computer Applications and Computer Science and Engineering. She holds a \\nDoctorate degree in Information and Communication Engineering f rom Anna \\nUniversity, Chennai. She has co-authored a book on data structu res under the \\nWiley Publications. He r subject expertise and interest include data structures \\nand algorithms, algorithm design t echniques and data analytics.  \\nR. Venkatesan is currently a Professor and the Head of the Depa rtment of CSE \\nat PSG College of Technology, Coi mbatore. He obtained his BE (H ons) in \\nMechanical Engineering and ME in Industrial Engineering from Ma dras \\nUniversity. He acquired his MS i n Computer and Information Scie nce from the \\nUniversity of Michigan, before completing his PhD from Anna Uni versity, \\nChennai under the Faculty of Inf ormation and Communication Engi neering. He \\nhas 18 years of industry experience, where he actively coordina ted and \\nmanaged large-scale software projects. He is also a member of t he Institute of \\nEngineers of India. His areas of interest include software engi neering, data \\nstructures, algorithm design and object oriented systems. \\nGirish Pasupathy is a Software Engineer at Microsoft India Deve lopment \\nCenter, Hyderabad. He received his Bachelor degree in Computer Science and \\nEngineering from PSG College of Technology, Coimbatore. His are as of \\ninterest include computer network ing and distributed computing.  \\nP. Swaradh is the Head of the Department of Computer Science an d \\nEngineering at KMCT College of E ngineering, Calicut and has an experience \\nof eight years in the academic field. He obtained his BTech in Information \\nTechnology from Calicut Universit y and ME in Computer Science a nd \\nEngineering from PSG College of Technology, Coimbatore. His sub ject \\nexpertise includes data mining, information retrieval, data int ensive computing \\nand data structures. \\n \\n1 Introduction \\nThe advent of microblogging sites has seen the proliferation of  emotions being exhibited \\nin the most unconventional of ways. Twitter is one of the most popular microblogging \\nplatforms which averaged at 304 million monthly active users wi th an average of  \\n500 million tweets per day as of the second quarter of 2015. Tw itter had thus established \\na platform for people to express their thoughts in short messag es called tweets. Thus, \\nthousands of tweets are published per second thereby building a  huge repository of \\n‘thoughts’. Emotions underlie most of the thoughts and are cruc ial to gain insight into the \\nhuman mind to further the business, health, security and other interests. Research studies \\nhave been undertaken in identifying emotions in stories, music,  blogs, spoken data and \\nmicro blogs using textual content, physiological signals, facia l expressions and speech. \\nPrevious work has classified emotions on Ekman’s (1992) six bas ic emotions of anger, \\ndisgust, fear, happiness, sadness, and surprise or Plutchik’s ‘ wheel of 8 basic emotion’ \\nwhich includes trust and anticipation. The purpose of this work  i s  t o  i m p r o v e  t h e  \\nclassification accuracy of the tweets into happy, sad, anger, f ear, disgust and surprise ', metadata={'source': 'journal paper.pdf', 'page': 1}), Document(page_content='   \\n    \\n   \\n    \\n   \\n    \\n    A lexicon-based term weighting scheme for emotion identification of tweets 371    \\n \\n \\n    \\n     \\n   \\n    \\n   \\n    \\n       \\n using machine learning with impro ved tweet effective preprocess ing and appropriate use \\nof lexicons and feature weighting. \\nIn this paper, preprocessing is extensively done using the rich  lexicons and tools \\navailable to convert the tweet vocabulary to the English vocabu lary. The comprehensive \\nnature of preprocessing was found to significantly improve the accuracy. In addition to \\nearlier work in emotion identification, interjections and punct uation marks were handled \\nseparately and also a new class called No_Emotion was added to improve the \\nclassification accuracy. Affectiv e words are words exhibiting e motions and valence \\nquantifies the amount of emotion. Researchers performing emotio n identification have \\nextensively used affective word s as features (Mohammad, 2012b).  T o  a i d  i n  t h e  \\nclassification, the feat ure vector was created from the availab le emotion lexicons and \\nthesaurus.com. To eliminate fals e classifications negations wer e handled using negations \\nwords list. The feature vector formed with the affective words along with a weighting \\nscheme based on presence of the tweet term in the feature vecto r and the presence of \\npunctuations and negation was used to train three machine learn ing classifiers namely \\nSVM, random forests, and naive Bayes. \\n2 Related work \\nTextual and non-textual data exhibit varied emotions. Emotions can be identified by \\nanalysing the data. It helps to improve business and government  prospects by identifying \\npublic sentiment towards products, services, policies and event s (Qadir and Riloff, 2014). \\nResearch has been carried out in identifying emotions from reso urces with considerable \\ntextual content with grammatically correct sentences like stori e s  ( A l m  e t  a l . ,  2 0 0 5 ;  \\nMohammad, 2011) to resources like tweets (Cambria et al., 2013;  Mohammad  \\net al., 2014; Guthrie et al., 2012) and chat messages (Ishizuka  et al., 2005; Holzman and \\nPotteneger, 2003) with limited  text (Dubey et al., 2016). \\nExisting methods for emotion identification have used naive and  sophisticated \\napproaches using lexicons and mach ine learning. A useful and fa irly successful naive \\napproach is the affec tive word detection and subsequent classif ication based on the \\nfrequency of occurrence of the affective words (Cardie et al., 2005). This notion that \\naffective words are differentia ting factors than non-affective words has been used in this \\npaper to reduce the size of the feature vector. Sentences may b e composed of conflicting \\naffective words and negations like ‘not’. For example, ‘The ven ture started on a joyful \\nnote, but things are going bad now’ exhibits ‘sadness’ though j oyful has a strong \\naffective valence and exhibits ‘happiness’. \\nWordNet-Affect is a lexicon with emotion labels for emotion exh ibiting WordNet \\nsynsets.10 ConceptNet is a semantic network that relates concep ts rather than just mere \\nwords as per human understanding in a machine readable form (Ha vasi and Speer, 2013). \\nBoth the lexicons have been widely used in identifying emotions  (Cambria et al., 2013; \\nChen et al., 2012). AffectiveSp ace was created by combining Con ceptNet and  \\nWordNet-Affect to associate emo tions with concepts (Cambria et al., 2013). Chen et al. \\n(2012) used WordNet-Affect to form the feature vector with the frequency of the \\naffective words under each emotion subcategory. Mohammad (2012b ) used the 1,000 \\nheadline lexicon, TEC lexicon, Wo rdNet-Affect lexicon and the N RC emotion lexicon \\nand showed that affect lexicons help to produce better results.  Our work extensively used ', metadata={'source': 'journal paper.pdf', 'page': 2}), Document(page_content='   \\n    \\n   \\n    \\n   \\n    \\n   372 S.L. Rose et al.    \\n \\n    \\n     \\n   \\n    \\n   \\n    \\n       \\n the affective words from the emotion lexicons and also used the m  a s  s e e d  w o r d s  t o  \\nimprove the feature vector. \\nStudies on the effect of the n-gram model on emotion identifica tion found that the \\nmodel does not improve the classification accuracy and does not  scale well for new \\ndomains (Chen et al., 2012; Mohammad, 2012b). Bigrams and trigr ams are found to be \\nhelpful in cases where positive affective words exhibit negativ e emotions and vice versa \\ndue to the presence of negation. POS features like adjectives a nd adverbs that improve \\nsentiment analysis are not useful for emotion identification (C hen et al., 2012). Rather \\nthey are used as seeds to improve the affective words list. Has htags, emoticons and emoji \\nhave been used to collect annotated data and have been shown to  hoard emotions and our \\nwork takes advantage of the affective nature of hashtags and em oticons in classification \\n(Ide and Suttles, 2013; Mohamma d, 2012a; Qadir and Riloff, 2014 ; Hoecke et al., 2014). \\nSupervised machine l earning-based classification techniques lik e naive Bayes, \\nmaximum entropy, SVM, k-NN, DNN , CNN, liblinear, multinomial na ive Bayes and \\nlogistic regression have been used in earlier work (Cambria et al., 2013; Ide and Suttles, \\n2013; Cardie et al., 2005; Mohammad, 2012b; Qadir and Riloff, 2 014; Cambria et al., \\n2013). Mohammad (2012b) observed that for emotion classificatio n, supervised methods \\nbehave better than unsupervised methods. Our choice was restric ted to SVM, naive Bayes \\nand random forest due to the nature of the feature vector. Earl ier term weighting \\nalgorithms used frequency, position, scattering, topicality (Ja hnavi and Radhika, 2015). A \\nnovel term weighting scheme based on punctuations and negation has been proposed in \\nthis paper. It is impossible to accurately identif y the emotion s in tweets without proper \\npreprocessing. Some of the preprocessing steps followed in hand ling the tweet language \\ninclude changing the case, POS tag ging, handling words with rep eating letters by \\nreplacing the repeating letter with exactly two letters, replac ing informal expressions with \\nformal expressions, stripping hash symbols and handling onomato poeias, special \\npunctuation, exclamation words, negations, adverbs and emoticon s  w i t h  t h e  h e l p  o f  \\nlexicons (Cambria et al., 2 013; Cardie et al., 2005). \\n3 Preprocessing and feature vector generation \\n3.1 Preprocessing \\nTwitter limits the number of characters in a tweet to 140. This  has facilitated people to \\nimprovise by writing compressed versions of the natural languag e, necessitating the \\npreprocessing of the tweets for further processing. For instanc e, consider the tweet ‘dis \\nbook looks so gr8!!! Its ossom’. Without preprocessing, if the tweet was subjected to \\nemotion analysis, the result would be No_Emotion. But the words  ‘gr8’ and ‘ossom’ \\nexhibit emotions and it is important to preprocess them. \\nInternet slangs like letter homophones which include acronyms ( e.g., AFAIK –  \\nAs Far As I Know) and abbreviations (e.g., gr8 – great), Onomat opoeic spellings (e.g., \\nhahaha), keyboard generated emoticons and smileys (e.g., :)), p unctuations and \\nexclamations (e.g., !!!!) are ex tensively used. Identifying the se slangs and converting \\nthem to natural language is the first step towards preprocessin g and slang dictionaries are \\nused for this purpose. \\n ', metadata={'source': 'journal paper.pdf', 'page': 3}), Document(page_content='   \\n    \\n   \\n    \\n   \\n    \\n    A lexicon-based term weighting scheme for emotion identification of tweets 373    \\n \\n \\n    \\n     \\n   \\n    \\n   \\n    \\n       \\n Hashtags are handled by removing the ‘#’ symbol and retaining t he words which are \\nusually loaded with emotions15. To stress the emotion behind a word, repeated characters \\nlike happppy for happy are normally found in tweets. These are mapped to their standard \\nform using regular expressions. \\nInterjections like tgif (Thank God, Its Friday) are rich in emo tions and are handled \\nseparately and not as acronyms. They are not expanded but rathe r tagged as interjections \\nfor further processing using an interjection dictionary. Emotic ons have a heavy affective \\nvalence and are replaced with their respective emotion using an  emoticons dictionary \\n(Bravo-Marquez et al., 2016). The emoticons dictionary has the list of emoticons and \\ntheir corresponding emotion. The tweet after the preprocessing mentioned above is \\nreferred to as the normalised tweet. The normalised tweets are tokenised and tagged with \\nthe part of speech and type as given in Table 1 with the help o f Tokeniser from CMU ark \\nTweetNLP (Bravo-Marquez et al., 2016). \\nTable 1  Parts of speech and tags \\nNoun – N Emoticon – E Proper noun verbal – M \\nVerb – V Hashtag – # Nominal possessive – S \\nAdjective – A Interjection – ! Discourse marker – ~ \\nAdverb – R Punctuation – , Nominal verbal – L \\nPronoun – O Number – $ Existential – X Conjunction – & Mention – @ Existential verbal – Y \\nDeterminer – D Abbreviation – G Proper noun possessive – Z \\nPreposition – P Verb particle – T  Proper noun – ^ URL – U  \\nThe tweet “i am soooooooo excited of india winning icc world cu p :))))))))) \\n#happpppppppppy” would be preprocessed and tagged as given belo w. \\ni[0] am[V] soo[R] excited[A] of[P] india[^] winning[V] icc[^] w orld[N] cup[N] :)[E] \\nomg[!] #happy[#]. \\nNegation words like never, no, not and so on are collected from  various sources to form \\nthe negation words list. These words can change the emotion exh ibited by the affective \\nwords and decrease effective id entification of the emotions. Fo r example, the tweet “I am \\nnot happy” exhibits a sad emotion with a high valence even with  the presence of the \\naffective word ‘happy’ (Cambria, 2016). \\nAmong the numerous punctuation marks, exclamation and question marks are \\nconsidered for emotion identifi cation. It has been found that p eople often use exclamation \\nmarks to indicate strong feelings and to mark the end of a sent ence. It was observed from \\nthe development set that, a series of exclamation and question marks are used to express \\nstrong feelings in tweets. For example, in the tweets “Some of my followers are starting \\nto take the bold step and unprotect their tweets!! #Proud!!” an d “@Brooke2591 when are \\nwe going to get together!!???? #curious #anxious”, the punctuat ion marks boost the \\nemotion of the sentence in which they occur. In particular, bot h exclamation and question \\nmark emphasise the emotion of the affective words around them. ', metadata={'source': 'journal paper.pdf', 'page': 4}), Document(page_content='   \\n    \\n   \\n    \\n   \\n    \\n   374 S.L. Rose et al.    \\n \\n    \\n     \\n   \\n    \\n   \\n    \\n       \\n 3.2 Feature vector generation \\nEmotion Lexicon could be thought of as a dictionary which consi sts of words and their \\nemotion categories. Involving affective lexicons improve the ac curacy of the system \\n(Sulis et al., 2016). Available emotion lexicons were used for the identification of Paul \\nEkman’s basic emotions in tweets. It was found that in a tweet the words that are used to \\nexpress emotions fall under on e of these categories namely \\nx Noun (Ex – excitement, happiness) \\nx Verb (Ex – hate, love) \\nx Adjective (Ex –beautiful, happy) \\nx Adverb (Ex – beautifully, cheerfully) \\nx Emoticons (Ex – smiling) \\nx Hashtags (Ex – #Happy, #disgust) \\nx Interjections [Ex – ugh (represen ts disgust), yay (represents j oy)]. \\nThe feature vector consists of 10,577 entries exhibiting the em otions happy, sad, anger, \\nfear, disgust and surprise. The feature vector was built with w ords from the following \\nsources. \\nThe NRC emotion lexicon has manually tagged 14,000 words. Each word is mapped \\nto zero or more of the eight emotions namely anger, fear, antic ipation, trust, surprise, \\nsadness, joy, and disgust and to one of the two sentiments name ly positive and negative \\n(Mohammad, 2012a; Bravo-Marquez et al., 2016). \\nWordNet-Affect is a linguistic r esource that has affective word s annotated with their \\naffective labels. To further imp rove the feature vector, the af fective words were taken as \\nseed words and their synonyms were queried at Thesaurus.com. Th e words from NRC \\nemotion list, WordNet-Affect and their synonyms form the final feature vector. \\n4 Lexicon-based term weighting \\nThe feature vector consists of t he 10,577 affective words in th e emotion lexicon \\nrepresented as \\n\\x0b\\x0c12 n Fv , v , , v …  …  ( 1 )  \\nThe typical tweet can  be formulated as \\n\\x0b\\x0c12 m Tt , t , , t …  …  ( 2 )  \\nwhere each t k is a tweet term. \\nThe mapping g: T → F is such that every term in T is mapped to a term in F with or \\nwithout stemming. The weight w i of feature vect or vi is assigned as follows. \\n ', metadata={'source': 'journal paper.pdf', 'page': 5}), Document(page_content='   \\n    \\n   \\n    \\n   \\n    \\n    A lexicon-based term weighting scheme for emotion identification of tweets 375    \\n \\n \\n    \\n     \\n   \\n    \\n   \\n    \\n       \\n Algorithm Term Weighting (T, F) \\n1 Initialise w i ← 0 for every v i in F. \\n2 For every term t j in T \\n3  If t j = vi for some i \\n4  w i ← w i + 1 \\n5  If ! or ? precedes t j \\n6  w i ← w i + 1 \\n7  End if \\n8  If negation precedes tj 9  w\\ni ← w i – 2//Decrease by –1 \\n10  End if \\n11  End if 12 End for \\nSince the size of tweet is 140 characters, a tweet can at most contain 20 features words. \\nThus at most only 20 entries in the feature vector will be set and the others remain as 0. \\nThis shows the sparseness of the feature vector. \\n5 Experimental evaluations \\nTweets along with their emotion are required for developing, tr aining and testing \\nmachine learning-based classifie rs. For the classifier to achie ve good accuracy, a large \\nemotion annotated dataset would be ideal. For the purpose of ex perimentation, the tweet \\ndataset was extracted from Twitter using seed words consisting of emotion terms and \\ntheir synonyms. Dataset corresponding to No_Emotion class was c reated manually using \\nfacts which do not have any emotions. The tweet corpus had 5,91 1 tweets and tweet IDs \\nunder the categories Happy, Sad, Anger, Fear, Disgust, Surprise  and No_Emotion. The \\nemotion distribution in the training set is as follows: \\nTable 2  Distribution of emotions in training and testing datasets \\nEmotion category Number of tweets in training \\ndataset Number of tweets in testing  \\ndataset \\nHappy 621 309 \\nSad 598 269 \\nAnger 580 276 \\nFear 607 273 Disgust 610 273 \\nSurprise 580 294 \\nNo_Emotion 350 271 Total 3,946 1,965 ', metadata={'source': 'journal paper.pdf', 'page': 6}), Document(page_content='   \\n    \\n   \\n    \\n   \\n    \\n   376 S.L. Rose et al.    \\n \\n    \\n     \\n   \\n    \\n   \\n    \\n       \\n 5.1 Classification techniques used \\nThere are numerous classifiers and in this paper naive Bayes, S VM and random forests \\nhave been considered due to the characteristics of the feature vector. We used the WEKA \\nimplementation of these machine learning algorithms. The Weka s oftware is Java-based \\nand consists of Java packages consisting of a collection of mac hine learning algorithms \\nfor data mining tasks. The feature vector is of a high dimensio n due to the presence of \\nmore than 10,000 feature vectors and is sparse as seen in previ ous samples. To train \\nSVM, the SMO learning algorithm is used with polynomial kernel.  Random forest is a \\nclassifier that constructs numerous decision trees with control led variations and then \\noutputs as class the mode of the class of the various decision trees. The emotion \\ndistribution in the training dataset is unbalanced as shown in Table 2 and random forest is \\na good choice to perform classification on such data. The rando m variable selection \\nparameter in random forests was set at log 2(m) + 1 for a feature size of ‘m’. For \\ncomparison purpose, naive Bayes was also used. Naive Bayes is a  simplistic algorithm \\nfound to work well in categorising text and has been found to w ork considerably well for \\nclassifying tweets into positive and negative sentiments. \\n5.2 Experimental setup and evaluation metrics \\nThe experimentation was performed on an Intel Xeon E3 processor  wi th 16 GB RAM \\nand on Windows 7 platform. Java was used for preprocessing and to handle the lexicon \\nclassifiers, while Weka 3.7 was used to train and classify the machine learning classifiers. \\nThe classified emotions are evalu ated using the f ollowing metri cs. \\nx Precision score:  Computes the fraction of number of tweets identified correctly  for \\nan emotion and number of tweets identified as belonging to an e motion. \\nx Recall score:  Computes the fraction of number of tweets identified correctly  for an \\nemotion and number of tweets that should have been identified a s belonging to an \\nemotion. \\nx F-measure:  Both precision and recall is a v alue between 0 and 1 and is ca lculated for \\nevery class. The harmonic mean b etween the precision and recall  scores gives the  \\nF-measure. \\nx Accuracy:  Computes the fraction between the number of tweets correctly c lassified \\nas belonging to an emotion and th e total number of tweets class ified. \\n6 Results and discussions \\nThe evaluation metrics for the SVM classifier, random forests a nd naïve Bayes are given \\nin Table 3. \\n \\n \\n ', metadata={'source': 'journal paper.pdf', 'page': 7}), Document(page_content='   \\n    \\n   \\n    \\n   \\n    \\n    A lexicon-based term weighting scheme for emotion identification of tweets 377    \\n \\n \\n    \\n     \\n   \\n    \\n   \\n    \\n       \\n Table 3  Precision, recall, F-measure for SVM \\nEmotions Precision Recall F-measure \\nHappy 0.7735 0.8918 0.8282 \\nSad 0.5836 0.8820 0.7019 \\nAnger 0.6992 0.8616 0.7719 Fear 0.6264 0.8143 0.7077 \\nDisgust 0.5861 0.6838 0.6312 \\nSurprise 0.6361 0.9397 0.7584 No_Emotion 0.9779 0.4064 0.5738 \\nTable 4  Precision, recal l, F-measure for random forests \\nEmotions Precision Recall F-measure \\nHappy 0.7735 0.9088 0.8354 \\nSad 0.5836 0.7982 0.6738 \\nAnger 0.6992 0.8249 0.7567 Fear 0.6264 0.8650 0.7263 \\nDisgust 0.5861 0.6420 0.6127 \\nSurprise 0.6361 0.8220 0.7171 No_Emotion 0.9779 0.4962 0.6584 \\nTable 5  Precision, recall, F-measure for naïve Bayes \\nEmotions Precision Recall F-measure \\nHappy 0.9288 0.9053 0.9168 \\nSad 0.6022 0.8853 0.7166 \\nAnger 0.7137 0.8640 0.7819 \\nFear 0.6117 0.9542 0.7457 Disgust 0.4872 0.7644 0.5948 \\nSurprise 0.5748 0.9135 0.7059 \\nNo_Emotion 0.9594 0.3698 0.5350 \\nThe purpose of this work is to classify the tweets according to  its emotion. The best \\naccuracy was achieved by random forest classifier and the least  was obtained for SVM, \\nwith the accuracy of naive Baye s being slightly above the SVM c lassifier. While happy is \\nthe easiest emotion to classify, the other emotions have a redu ced precision due to the \\ninherent ambiguity in the emotion words. For example, cringe ca n be used to express \\nsadness or fear. No_Emotion class has a higher precision due to  the absence of emotion \\nwords in the tweets. The low recall in the No_Emotion class can  be attributed to the stray \\naffective words which were present in the tweets belonging to t hat class. For example, the \\nfact ‘Everybody wants to win’ wa s misclassified as happy due to  the presence of ‘win’. \\n   ', metadata={'source': 'journal paper.pdf', 'page': 8}), Document(page_content='   \\n    \\n   \\n    \\n   \\n    \\n   378 S.L. Rose et al.    \\n \\n    \\n     \\n   \\n    \\n   \\n    \\n       \\n The graph in Figure 1 shows the accuracy of each emotion class happy, sad, anger, \\nfear, disgust, surprise, and No_Emotion with respect to each of  the three classifiers. \\nFigure 1  Classification accuracy of emotion categories (see online vers ion for colours) \\n \\nThe comparison of the macro averaged precision, recall and F-me asure values \\ncorresponding to each c lassifier is given in Figure 2. From the  analysis, the result shows \\nthat the extensive preprocessing and addition of the new No_Emo tion class performs well \\nto attain better classification accuracy. \\nFigure 2  Comparison of macro-average precision, recall and F-measure (s ee online version  \\nfor colours) \\n ', metadata={'source': 'journal paper.pdf', 'page': 9}), Document(page_content='   \\n    \\n   \\n    \\n   \\n    \\n    A lexicon-based term weighting scheme for emotion identification of tweets 379    \\n \\n \\n    \\n     \\n   \\n    \\n   \\n    \\n       \\n The difference between the machin e learning classifiers lie not  only in accuracy, but also \\nin the memory size of the model files build by each classifier.  While the model file for \\nthe SMO classifier occupied only 1,251 KB, the model file for t he naive Bayes occupied \\n5,280 KB, where as the model file for the random forests algori thm used a whopping \\n20,777 KB. The stack size of the java project had to be increas e d  t o  1 0  M B  f r o m  t h e  \\ndefault size to run the random forest classifier. \\n7 Conclusions \\nEmotion classification in tweets is a highly challenging task d ue to the nature of the text \\navailable. To achieve good accur acy, an exhaustive preprocessin g technique and  \\nlexicon-based feature selection is proposed. We map the twitter  content to the lexicon to \\nimprove the efficiency of the m achine learning classifier and a chieve a good accuracy. \\nThe involvement of interjections, synonyms from lexicon, negati ons words and handling \\nof the punctuation marks improved  the accuracy of the machine l earning classifiers. The \\nfeature vector created from NRC emotion lexicon, WordNet-Affect  a n d  t h e s a u r u s  w a s  \\nused to considerably reduce the dimension of the feature vector s for the machine learning \\nclassifiers. The addition of the new No_Emotion class for class ifying non-emotional \\ntweets was also found to reduce the misclassification error. In  future, emotion classes \\nunique to twitter data are to be researched and the feature vec tor is to be extended to \\ninclude the affective words of the other emotion classes as wel l. The effect of the order of \\noccurrence of the affective word s in emotions and the consequen ce and the valence of the \\nhashtags are to be learnt and used to improvise the classifiers . Using lexicons to improve \\nterm weighting has not been researched much and research may be  done using the  \\nwell-established affective lexical resources like ANEW, Wordnet -Affect, SentiWordNet \\nand General Inquirer to check its effect on the classification accuracy. \\nReferences \\nAlm, A.C.O., Roth, D. and Spro at, R. (2005) ‘Emotions from text : machine learning for text-based \\nemotion prediction’, Proc. Conference on Human L anguage Technology and Empirical \\nMethods in Natural Language Processing , pp.579–586. \\nBravo-Marquez, F., Frank, E. and Pfahringer, B. (2016) ‘Buildin g a Twitter opinion lexicon from \\nautomatically-annotated tweets’, Knowledge-Based Systems , Vol. 108, pp.65–78. \\nCambria, E. (2016) ‘Affective com puting and sentiment analysis’ , IEEE Intelligent Systems ,  \\nVol. 31, No. 2, pp.102–107. \\nCambria, E., Hussain, A. and Mazzocco, T. (2013) ‘Application o f multi-dimensional scaling and \\nartificial neural networks for biologically inspired opinion mi ning’, Biologically Inspired \\nCognitive Architectures , Vol. 4, pp.41–53. \\nCardie, C., Wiebe, J. and Wils on, T. (2005) ‘Annotating express ions of opinions and emotions in \\nlanguage’, Language Resources and Evaluation , Vol. 39, Nos. 3/4, pp.165–210. \\nChen, L., Sheth, A.P., Thirunarayan, K. and Wang, W. (2012) ‘Ha rnessing Twitter Big Data for \\nautomatic emotion identification’, Proc. ASE/IEEE International Conference on Social \\nComputing , pp.587–592. \\nDubey, G., Rana, A. and Ranjan, J. (2016) ‘A research study of sentiment analysis and various \\ntechniques of sentiment classification’, International Journal of Da ta Analysis Techniques and \\nStrategies , Vol. 8, No.  2, pp.122–142. ', metadata={'source': 'journal paper.pdf', 'page': 10}), Document(page_content='   \\n    \\n   \\n    \\n   \\n    \\n   380 S.L. Rose et al.    \\n \\n    \\n     \\n   \\n    \\n   \\n    \\n       \\n Ekman, P. (1992) ‘An argument for basic emotions’, Cognition & Emotion , Vol. 6, Nos. 3/4, \\npp.169–200. \\nGuthrie, J., Harabagiu, S.M., Johnson, J., Roberts, K. and Roac h, M.A. (2012)  ‘EmpaTweet: \\nannotating and detecting emotions on Twitter’, Proc. 8th International Conference on \\nLanguage Resources and Evaluation . \\nHavasi, C. and Speer, R. (2013) ConceptNet 5: A Large Semantic Network for Relational \\nKnowledge, Theory and Applications  of Natural Language Processing , Springer. \\nHoecke, S.V., Janssens, O., Mannens, E., Verstockt, S. and de W alle, R.V. (2014) ‘Influence of \\nweak labels for emotion recognition of tweets’, Proc. 2nd International Conference on Mining \\nIntelligence and Know ledge Exploration , pp.108–118. \\nHolzman, L.E. and Pottenger, W.M. (2003) Classification of Emotions in Internet Chat: An \\nApplication of Machine Learning Using Speech Phonemes , Technical Report, Leigh \\nUniversity. \\nIde, N. and Suttles, J. (2013) ‘Distant supervision for emotion  classification with discrete binary \\nvalues’, Proc. 14th International C onference on Computational Linguistics and Intelligent \\nText Processing , pp.121–136. \\nIshizuka, M., Ma, C. and Prendi nger, H. (2005) ‘Emotion estimat ion and reasoning based on \\naffective textual interaction’, Proc. 1st International Confer ence on Affective Computing and \\nIntelligent Interaction , pp.622–628. \\nJahnavi, Y. and Radhika, Y. (2015)  ‘FPST: a new term weighting algorithm for long running and \\nshort lived events’, International Journal of Data Analysis Techniques and Strategies , Vol. 7, \\nNo. 4, pp.366–383. \\nMohammad, S. (2011) ‘From once upon a time to happily ever afte r: tracking emotions in novels \\nand fairy tales’, Proc. 5th ACL-HLT Workshop on Language Technology for Cultural \\nHeritage, Social Sciences, and Humanities , pp.105–114. \\nMohammad, S. (2012a) ‘#Emotional tweets’, Proc. 1st Joint Confer ence on Lexical and \\nComputational Semantics , pp.246–255. \\nMohammad, S. (2012b) ‘Portable feat ures for classifying emotion al text’, Proc. North American \\nChapter of the Association for Computa tional Linguistics: Human Language Technologies , \\npp.587–591. \\nMohammad, S.F., Martin, J. and Z hu, X. (2014) ‘Semantic role la beling of emotions in tweets’, \\nProc. 5th Workshop on Computational Approac hes to Subjectivity, Sentiment, and Social \\nMedia Analysis , pp.32–41. \\nQadir, A. and Riloff, E. (2014) ‘Learning emotion indicators fr om tweets: hashtags, hashtag \\npatterns, and phrases’, Proc. 2014 Conference on Empirical Methods in Natural Language \\nProcessing , pp.1203–1209. \\nStrapparava, C. and Valitutti, A . (2004) ‘WordNet-Affect: an af fective extension of WordNet’, \\nProc. 4th International Conference on Language Resources and Evaluation , pp.1083–1086. \\nSulis, E., Farías, D.I.H., Rosso, P., Patti, V. and Ruffo, G. ( 2016) ‘Figurative messages and affect \\nin Twitter: differences between  #irony, #sarcasm and #not’, Knowledge-Based Systems , \\npp.132–143. ', metadata={'source': 'journal paper.pdf', 'page': 11})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the OpenAIEmbeddings class<br>\n",
        "The OpenAIEmbeddings class allows you to generate vector embeddings for text using OpenAI's embedding models.\n",
        "\n"
      ],
      "metadata": {
        "id": "1gTL_rAsCJfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings_model = OpenAIEmbeddings(openai_api_key=openai.api_key)"
      ],
      "metadata": {
        "id": "SYcQeUVtCxKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Document QA<br>\n",
        "<li>Split the documents\n",
        "<li>Embed with OpenAIEmbeddings\n",
        "<li>Store them in a Vectorstore index automatically.<br><br>\n",
        "\n",
        "The key steps are:\n",
        "\n",
        "<li>Load the PDF using PyPDFLoader into a list of Document objects\n",
        "\n",
        "<li>Initialize the OpenAIEmbeddings model\n",
        "\n",
        "<li>Pass the embeddings model to the VectorstoreIndexCreator\n",
        "\n",
        "<li>Call .from_loaders() on the index creator, passing in the PyPDFLoader"
      ],
      "metadata": {
        "id": "cg_i3O3JCfe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Vectorstore index is a search index that allows fast semantic search over text documents. It is created using the VectorstoreIndexCreator class in LangChain."
      ],
      "metadata": {
        "id": "oS5-bn3rE3Mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai.api_resources import embedding\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "\n",
        "index = VectorstoreIndexCreator(embedding=embeddings_model).from_loaders([loader])"
      ],
      "metadata": {
        "id": "T4SJ9-BR2hUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the resulting index object to run semantic search queries over the PDF contents."
      ],
      "metadata": {
        "id": "IhWzH370DLwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index.query(llm=llm,question=\"Who are the authors of the paper\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fj0Hzd67R_DM",
        "outputId": "db928019-80c9-4f13-9692-2ae0b382ac75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The authors of the paper are S.L. Rose, R. Venkatesan, G. Pasupathy, and P. Swaradh.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index.query(llm=llm,question=\"What are their affiliations\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "ljKlxVxPEePM",
        "outputId": "ac9a7197-6214-40fd-de58-b0f268431110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'S. Lovelyn Rose is an Associate Professor at the Department of CSE in PSG College of Technology, Coimbatore. \\nR. Venkatesan is currently a Professor and the Head of the Department of CSE at PSG College of Technology, Coimbatore. \\nGirish Pasupathy is a Software Engineer at Microsoft India Development Center, Hyderabad. \\nP. Swaradh is the Head of the Department of Computer Science and Engineering at KMCT College of Engineering, Calicut.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index.query(llm=llm,question=\"Summarize the methodology employed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "mc3S9lyjEtjs",
        "outputId": "ef1ea7dc-21bf-4b4e-d167-7cd25e2cc81c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The methodology employed in this study involved the use of classification techniques to identify emotions in tweets. The classifiers used were naive Bayes, SVM, and random forests. The feature vector used in the classification was high-dimensional and sparse. The WEKA software, which consists of machine learning algorithms, was used for implementing these classifiers. The experimental setup involved using an Intel Xeon E3 processor with 16 GB RAM and the Windows 7 platform. The evaluation metrics used to assess the performance of the classifiers included precision score, recall score, and F1 score.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index.query(llm=llm,question=\"List the refernce papers between the years 2012 to 2015\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "-sAV0FN7FJYs",
        "outputId": "670b09db-00f0-4a8c-baba-23cfbd76c593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The reference papers between the years 2012 to 2015 are:\\n\\n1. Ide, N. and Suttles, J. (2013) ‘Distant supervision for emotion classification with discrete binary values’, Proc. 14th International Conference on Computational Linguistics and Intelligent Text Processing, pp.121–136.\\n\\n2. Jahnavi, Y. and Radhika, Y. (2015) ‘FPST: a new term weighting algorithm for long running and short-lived events’, International Journal of Data Analysis Techniques and Strategies, Vol. 7, No. 4, pp.366–383.\\n\\n3. Mohammad, S. (2012a) ‘#Emotional tweets’, Proc. 1st Joint Conference on Lexical and Computational Semantics, pp.246–255.\\n\\nPlease note that these are the only reference papers mentioned in the given context between the years 2012 to 2015.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index.query(llm=llm,question=\"What is the sample used\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "ZHejqhBDDSNm",
        "outputId": "e3c33d5d-048d-4f27-c5b5-c1f0a4339792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The sample used in this study consists of 3,946 tweets in the training dataset and 1,965 tweets in the testing dataset. The tweets are categorized into different emotions such as Happy, Sad, Anger, Fear, Disgust, Surprise, and No_Emotion.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index.query(llm=llm,question=\"What is the research investigating\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7Rn8Pq58EMCG",
        "outputId": "a5ed6edc-a8cd-485e-9535-d445f8c42e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The research is investigating the classification of emotions in tweets.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index.query(llm=llm,question=\"What are the implications of the results? Give in 2 sentences\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "QnBl9cCaFOur",
        "outputId": "a5683092-8ef8-4454-c4f4-66f9a7a1bdf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The results suggest that using n-gram models does not improve the accuracy of emotion identification and may not be suitable for new domains. Additionally, the inclusion of interjections, synonyms, negations, and punctuation marks can improve the accuracy of machine learning classifiers for emotion identification.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index.query(llm=llm,question=\"What experiments could be carried out to answer any further questions?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "Bb6qi_DnFZ9j",
        "outputId": "850ff0cf-bcd2-4d3b-882d-d2ae940c48ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the given context, some possible experiments that could be carried out to answer further questions could include:\\n\\n1. Experiment to compare different classification techniques: Conduct a study to compare the performance of different classifiers (e.g., naive Bayes, SVM, random forests) on classifying emotions in textual content. This could involve using a dataset with labeled emotions and evaluating the accuracy, precision, recall, and F-measure of each classifier.\\n\\n2. Experiment to evaluate the impact of preprocessing: Investigate the effect of extensive preprocessing on the classification accuracy of emotions. This could involve comparing the performance of classifiers on datasets with and without preprocessing steps such as removing stopwords, stemming, or normalizing text.\\n\\n3. Experiment to assess the addition of a new emotion class: Explore the impact of adding a new emotion class (e.g., No_Emotion) on the classification accuracy. This could involve comparing the performance of classifiers on datasets with and without the new emotion class and analyzing the differences in precision, recall, and F-measure.\\n\\n4. Experiment to analyze the influence of punctuation marks: Investigate the role of punctuation marks (e.g., exclamation and question marks) in emphasizing the emotion of affective words. This could involve analyzing a dataset of sentences with affective words and measuring the impact of different punctuation marks on the perceived emotion.\\n\\nThese experiments could provide insights into improving the classification accuracy of emotions and understanding the role of different factors in emotion detection.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index.query(llm=llm,question=\"What work have researchers previously done on this title. Format output  as bulleted list of points\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "LtSwsbCoG4Ij",
        "outputId": "4737f71e-3753-4280-dd35-31360faa6871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"- Researchers have used the affective nature of hashtags and emoticons in classification (Ide and Suttles, 2013; Mohammad, 2012a; Qadir and Riloff, 2014; Hoecke et al., 2014).\\n- Supervised machine learning-based classification techniques such as naive Bayes, maximum entropy, SVM, k-NN, DNN, CNN, liblinear, multinomial naive Bayes, and logistic regression have been used (Cambria et al., 2013; Ide and Suttles, 2013; Cardie et al., 2005; Mohammad, 2012b; Qadir and Riloff, 2014; Cambria et al., 2013).\\n- Mohammad (2012b) found that supervised methods perform better than unsupervised methods for emotion classification.\\n- Previous work has used term weighting algorithms based on frequency, position, scattering, and topicality (Jahnavi and Radhika, 2015).\\n- A novel term weighting scheme based on punctuations and negation has been proposed to improve the feature vector.\\n- Studies on the effect of the n-gram model on emotion identification found that it does not improve classification accuracy and does not scale well for new domains (Chen et al., 2012; Mohammad, 2012b).\\n- Bigrams and trigrams are found to be helpful in cases where positive affective words exhibit negative emotions and vice versa due to the presence of negation.\\n- POS features like adjectives and adverbs are not useful for emotion identification but are used as seeds to improve the affective words list (Chen et al., 2012).\\n- Hashtags, emoticons, and emoji have been used to collect annotated data and have been shown to capture emotions.\\n- Previous work has been done on identifying emotions in stories, music, blogs, spoken data, and microblogs using textual content, physiological signals, facial expressions, and speech.\\n- Emotions have been classified based on Ekman's six basic emotions (anger, disgust, fear, happiness, sadness, and surprise) or Plutchik's 'wheel of 8 basic emotions' which includes trust and anticipation.\\n- The purpose of this work is to improve the classification accuracy of tweets into happy, sad, anger, fear, disgust, and surprise.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}