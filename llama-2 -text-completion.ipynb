{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd1b144d",
   "metadata": {},
   "source": [
    "<b>Useful Links<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b440b764",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/integrations/llms/llamacpp#cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab01ff57",
   "metadata": {},
   "source": [
    "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/blob/main/llama-2-7b-chat.ggmlv3.q4_0.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d268f",
   "metadata": {},
   "source": [
    "https://ai.meta.com/llama/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92808b3",
   "metadata": {},
   "source": [
    "https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611bec3c",
   "metadata": {},
   "source": [
    "<b>Text Completion with Llama 2 Chat Model - No API</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58c42ed",
   "metadata": {},
   "source": [
    "<b>Llama 2 Model</b><br>\n",
    "<li>Open source large language model from Meta AI\n",
    "    <li>Collection of pretrained and fine-tuned generative text models\n",
    "        <li>Parameters : 7B, 13B, 70B\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291fa2f0",
   "metadata": {},
   "source": [
    "<b>Llama 2 Chat Model</b><br>\n",
    "<li>Fine-tuned LLM\n",
    "    <li>Optimized for dialogue use cases\n",
    "        <li>Outperform open-source chat models on most benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84c8d4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.1.77.tar.gz (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 3.2 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages (from llama-cpp-python) (4.6.3)\n",
      "Collecting diskcache>=5.6.1\n",
      "  Downloading diskcache-5.6.1-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 45.6/45.6 kB ? eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages (from llama-cpp-python) (1.24.4)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.77-cp38-cp38-win_amd64.whl size=1144486 sha256=d756caa1a90aaa0bd2b33a6942a2af41e7f16e177ad91d79f9d8256bdf0dd08c\n",
      "  Stored in directory: c:\\users\\lovelyn\\appdata\\local\\pip\\cache\\wheels\\2f\\9f\\ed\\e594208a93c49913b5e1b9dca7437b4e2a359fa34adee1c380\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.1 llama-cpp-python-0.1.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Run the LLaMA model using 4-bit integer quantization\n",
    "! pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb60294",
   "metadata": {},
   "source": [
    "<b>Quantization </b><br>\n",
    "Process of converting high-precision numerical values, such as 32-bit floating-point numbers, into lower-precision representations, like integers with fewer bits.\n",
    "\n",
    "<b>4-bit Quantization </b>\n",
    "<li> Specific type of integer quantization </li>\n",
    "<li> Reduces the precision of numerical values in a machine learning model to 4 bits.</li>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a0bc2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959b0b73",
   "metadata": {},
   "source": [
    "<b>StreamingStdOutCallbackHandler</b><br>\n",
    "Show the chain output in stdout(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6afe0ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: Find if the number is a prime number: {number}. \n",
    "\n",
    "Output : Give step by step explanation for the answer\n",
    "\n",
    "Answer: Inform if the number is prime or not prime\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be039c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "# Verbose is required to pass to the callback manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c277b5e",
   "metadata": {},
   "source": [
    "<b>Callback Function</b><br>\n",
    "Function that is passed as an argument to another function, and it is intended to be executed after the completion of that function or at a specific event trigger. <br><br>\n",
    "Callbacks are commonly used to handle asynchronous operations, such as \n",
    "<li>handling responses from web APIs</li>\n",
    "<li>processing events in graphical user interfaces</li>\n",
    "<li>handling data from asynchronous I/O operations</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca813204",
   "metadata": {},
   "source": [
    "<b>llama-2-7b-chat.ggmlv3.q4_0</b><br>\n",
    "    <li>7B</li>\n",
    "    <li>Optimized for dialogue use case\n",
    "    <li>Outperform other open-source chat models\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad6a74e",
   "metadata": {},
   "source": [
    "<b>ggml</b><br>\n",
    "<li> Tensor library for machine learning to enable large models and high performance on commodity hardware. \n",
    "    <li> Used by llama.cpp and whisper.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5acda17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"d:\\\\python code\\\\models\\\\llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "    input={\"temperature\": 0, \"max_length\": 2000, \"top_p\": 1},\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a81288d",
   "metadata": {},
   "source": [
    "List of different CPU instruction sets and their corresponding values. These instruction sets are used to enable specific hardware capabilities on CPUs, and their presence or absence determines the supported features of the processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3745b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "Input : The number 3\n",
      "\n",
      "Explanation : A prime number is a positive integer that is divisible only by 1 and itself. Since 3 can only be divided by 1 and 3, it is a prime number.\n",
      "\n",
      "Step 1: Check if the number is divisible by any other number except for 1 and itself.\n",
      "In this case, we need to check if 3 is divisible by any other number except for 1 and 3. Since 3 is not divisible by any other number except for 1 and 3, it is a prime number.\n",
      "\n",
      "Step 2: Check if the number is equal to 1 or itself.\n",
      "Since 3 is not equal to 1 or itself, it is a prime number.\n",
      "\n",
      "Therefore, the answer is : Yes, the number 3 is a prime number."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.\\n\\nInput : The number 3\\n\\nExplanation : A prime number is a positive integer that is divisible only by 1 and itself. Since 3 can only be divided by 1 and 3, it is a prime number.\\n\\nStep 1: Check if the number is divisible by any other number except for 1 and itself.\\nIn this case, we need to check if 3 is divisible by any other number except for 1 and 3. Since 3 is not divisible by any other number except for 1 and 3, it is a prime number.\\n\\nStep 2: Check if the number is equal to 1 or itself.\\nSince 3 is not equal to 1 or itself, it is a prime number.\\n\\nTherefore, the answer is : Yes, the number 3 is a prime number.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm,prompt = prompt)\n",
    "chain.run(\"3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631a78d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aa6ea6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "Answer: The antonym of \"good\" is \"bad\".\n",
      "The Tamil translation of \"good\" is \"நின்றாயிருப்பவர்\" (nintrāyiruppavar)."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'?\\n\\nAnswer: The antonym of \"good\" is \"bad\".\\nThe Tamil translation of \"good\" is \"நின்றாயிருப்பவர்\" (nintrāyiruppavar).'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Question: What is the antonym and translation(in Tamil) of good\n",
    "\"\"\"\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dbff797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solution:\n",
      "The square root of 3 is 1.732.\n",
      "\n",
      "Explanation:\n",
      "To find the square root of a number, we need to find a value that, when multiplied by itself, gives us the original number. In this case, 3 × 1.732 = 3, so 1.732 is the square root of 3."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nSolution:\\nThe square root of 3 is 1.732.\\n\\nExplanation:\\nTo find the square root of a number, we need to find a value that, when multiplied by itself, gives us the original number. In this case, 3 × 1.732 = 3, so 1.732 is the square root of 3.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Question: Find the square root of 3\n",
    "\"\"\"\n",
    "llm(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
