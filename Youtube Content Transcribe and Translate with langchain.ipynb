{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09216ff6",
   "metadata": {},
   "source": [
    "# YoutubeLoader\n",
    "### Input youtube video and Get transcribed content\n",
    "# LLMChain and OpenAI\n",
    "### Translate content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "044fe29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pytube in c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages (15.0.0)\n",
      "Requirement already satisfied: youtube-transcript-api in c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages (0.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages (from youtube-transcript-api) (2.26.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages (from requests->youtube-transcript-api) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages (from requests->youtube-transcript-api) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages (from requests->youtube-transcript-api) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages (from requests->youtube-transcript-api) (2021.10.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ordcloud (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\lovelyn\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install pytube youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a20bed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import YoutubeLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf9985f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=PWMrUCYnJEE\", add_video_info=True\n",
    ")\n",
    "youtube_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7195112d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"in this video I will give you a step-by-step guide on how to obtain responses from openai models using the langstain framework so first let's install Lang chain and open AI using pip install once you have installed long chain and open AI we need to import open AI from Lang Syne so this is the class which has the llm models in open AI now let's create an object for open AI first we authenticate the key using openai underscore API underscore key parameter then we can pass the model so the model that we use is text DaVinci model because we want a text completion and then finally I decide that the maximum number of tokens that I want as a response is only 20 and so I give Max tokens to be 20. the simplest way to use the llm model is by using the llm object we can call it with a very simple input text so I pass a string which is the prompt and it returns me a string which is a response this is the simplest in which I pass a text or a string and I get as output a string the second way is I can enhance the output by getting more information using the generate function so let me call the generate function on the llm object I can provide a list of prompts so here we find that the parameter is a list and I pass every single prompt as a string let's print the result that is generated Generations is a list and this has multiple sub lists now each sublist will represent one output of a romped so I have two generations sub lists here one to represent the output for the first prompt that is finding the acronym for Rosia and the second Supple is to find the acronym for API if I want to print the generated result for the first prompt I can access the generations attribute in llm result and this is a list and I retrieve the first element which is at index 0. so at index 0 I get the result of the first prompt it gives me the text and the generation information what if I want only the text now the zeroth index represents the first response which is the response for OCR in that I have only one response since the value of n is not specified and since it defaults to 1 and in that let me print the text similarly if I want the text for the second output I can give the index as 1. so make sure that you adjust the indices according to the requirement it is also possible to print information on the token usage and the model name which is stored in the attribute llm underscore output so I can access llm underscore output from llm underscore result\", metadata={'source': 'PWMrUCYnJEE', 'title': 'Step by Step Guide : Using openai models with langchain', 'description': 'Unknown', 'view_count': 29, 'thumbnail_url': 'https://i.ytimg.com/vi/PWMrUCYnJEE/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGC4gZSgwMA8=&rs=AOn4CLDdxE2JeygQixQFZ60knCMLQ3Nv-A', 'publish_date': '2023-07-01 00:00:00', 'length': 243, 'author': 'Lovelyn Rose'})]\n"
     ]
    }
   ],
   "source": [
    "print(youtube_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "526b04b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in this video I will give you a step-by-step guide on how to obtain responses from openai models using the langstain framework so first let's install Lang chain and open AI using pip install once you have installed long chain and open AI we need to import open AI from Lang Syne so this is the class which has the llm models in open AI now let's create an object for open AI first we authenticate the key using openai underscore API underscore key parameter then we can pass the model so the model that we use is text DaVinci model because we want a text completion and then finally I decide that the maximum number of tokens that I want as a response is only 20 and so I give Max tokens to be 20. the simplest way to use the llm model is by using the llm object we can call it with a very simple input text so I pass a string which is the prompt and it returns me a string which is a response this is the simplest in which I pass a text or a string and I get as output a string the second way is I can enhance the output by getting more information using the generate function so let me call the generate function on the llm object I can provide a list of prompts so here we find that the parameter is a list and I pass every single prompt as a string let's print the result that is generated Generations is a list and this has multiple sub lists now each sublist will represent one output of a romped so I have two generations sub lists here one to represent the output for the first prompt that is finding the acronym for Rosia and the second Supple is to find the acronym for API if I want to print the generated result for the first prompt I can access the generations attribute in llm result and this is a list and I retrieve the first element which is at index 0. so at index 0 I get the result of the first prompt it gives me the text and the generation information what if I want only the text now the zeroth index represents the first response which is the response for OCR in that I have only one response since the value of n is not specified and since it defaults to 1 and in that let me print the text similarly if I want the text for the second output I can give the index as 1. so make sure that you adjust the indices according to the requirement it is also possible to print information on the token usage and the model name which is stored in the attribute llm underscore output so I can access llm underscore output from llm underscore result\n"
     ]
    }
   ],
   "source": [
    "print(youtube_data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34e74625",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=PWMrUCYnJEE\"\n",
    ")\n",
    "youtube_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa6ef726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"in this video I will give you a step-by-step guide on how to obtain responses from openai models using the langstain framework so first let's install Lang chain and open AI using pip install once you have installed long chain and open AI we need to import open AI from Lang Syne so this is the class which has the llm models in open AI now let's create an object for open AI first we authenticate the key using openai underscore API underscore key parameter then we can pass the model so the model that we use is text DaVinci model because we want a text completion and then finally I decide that the maximum number of tokens that I want as a response is only 20 and so I give Max tokens to be 20. the simplest way to use the llm model is by using the llm object we can call it with a very simple input text so I pass a string which is the prompt and it returns me a string which is a response this is the simplest in which I pass a text or a string and I get as output a string the second way is I can enhance the output by getting more information using the generate function so let me call the generate function on the llm object I can provide a list of prompts so here we find that the parameter is a list and I pass every single prompt as a string let's print the result that is generated Generations is a list and this has multiple sub lists now each sublist will represent one output of a romped so I have two generations sub lists here one to represent the output for the first prompt that is finding the acronym for Rosia and the second Supple is to find the acronym for API if I want to print the generated result for the first prompt I can access the generations attribute in llm result and this is a list and I retrieve the first element which is at index 0. so at index 0 I get the result of the first prompt it gives me the text and the generation information what if I want only the text now the zeroth index represents the first response which is the response for OCR in that I have only one response since the value of n is not specified and since it defaults to 1 and in that let me print the text similarly if I want the text for the second output I can give the index as 1. so make sure that you adjust the indices according to the requirement it is also possible to print information on the token usage and the model name which is stored in the attribute llm underscore output so I can access llm underscore output from llm underscore result\", metadata={'source': 'PWMrUCYnJEE'})]\n"
     ]
    }
   ],
   "source": [
    "print(youtube_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dbb4ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5ae30cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an LLMChain to translate a transcript.\n",
    "llm = OpenAI(temperature=.7)\n",
    "template = \"\"\"Translate to {language}\n",
    "Text : {docs}\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"docs\",\"language\"], template=template)\n",
    "translate_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a68eab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = translate_chain.run(docs=youtube_data[0].page_content,language=\"french\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8dc69e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dans cette vidéo, je vous donnerai un guide étape par étape sur la façon d'obtenir des réponses à partir des modèles openai à l'aide du cadre Langstain. Alors, tout d'abord, installons Lang Chain et Open AI à l'aide de pip install. Une fois que vous avez installé Long Chain et Open AI, nous devons importer Open AI à partir de Lang Syne. C'est donc la classe qui a les modèles llm dans Open AI. Maintenant, créons un objet pour Open AI. Tout d'abord, authentifions la clé en utilisant le paramètre openai_API_key. Ensuite, nous pouvons passer le modèle. Le modèle que nous utilisons est le modèle Text DaVinci, car nous voulons une complétion de texte et, enfin, je décide que le nombre maximum de jetons que je veux comme\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
